{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import operator\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "from models.infvae_models import InfVAESocial, InfVAECascades\n",
    "from utils.preprocess import *\n",
    "from utils.flags import *\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = FLAGS.cuda_device\n",
    "\n",
    "\n",
    "flags = tf.compat.v1.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "# Set logs directory parameters.\n",
    "LOG_DIR = \"log/\"\n",
    "OUTPUT_DATA_DIR = \"log/output/\"\n",
    "ensure_dir(LOG_DIR)\n",
    "ensure_dir(OUTPUT_DATA_DIR)\n",
    "datetime_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "today = datetime.today()\n",
    "log_file = LOG_DIR + '%s_%s_%s_%s.log' % (FLAGS.dataset.split(\n",
    " \"/\")[0], str(today.year), str(today.month), str(today.day))\n",
    "\n",
    "#--\n",
    "\n",
    "def predict(session, model, feed):\n",
    "    \"\"\" Helper function to compute model predictions. \"\"\"\n",
    "    recall_scores, map_scores, n_samples, top_k, target = \\\n",
    "        session.run([model.recall_scores, model.map_scores, model.relevance_scores,\n",
    "                     model.top_k_filter, model.targets], feed_dict=feed)\n",
    "    return recall_scores, map_scores, n_samples.shape[0], top_k, target\n",
    "\n",
    "\n",
    "with ExpLogger(\"Inf-VAE\", log_file=log_file, data_dir=OUTPUT_DATA_DIR) as logger:\n",
    "    # log training parameters\n",
    "    try:\n",
    "        logger.log(FLAGS.flag_values_dict())\n",
    "    except:\n",
    "        logger.log(FLAGS.__flags.items())\n",
    "\n",
    "    ''' Load data: the datasets are expected to be pre-processed in an appropriate format. The assumption is that the \n",
    "    users appearing in cascades must also appear in the graph, while the converse may not be true. \n",
    "    Thus, the user indices are created based on the graph. '''\n",
    "    A = load_graph(FLAGS.dataset)\n",
    "    if FLAGS.use_feats:\n",
    "        X = load_feats(FLAGS.dataset)\n",
    "        # print(\"X的形状 %s\" % X.shape)\n",
    "    else:\n",
    "        X = np.eye(A.shape[0])\n",
    "\n",
    "    num_nodes = A.shape[0]\n",
    "    layers_config = list(map(int, FLAGS.vae_layer_config.split(\",\")))\n",
    "\n",
    "    if num_nodes % FLAGS.vae_batch_size == 0:\n",
    "        num_batches_vae = num_nodes // FLAGS.vae_batch_size\n",
    "    else:\n",
    "        num_batches_vae = num_nodes // FLAGS.vae_batch_size + 1\n",
    "\n",
    "    if FLAGS.graph_AE == 'GCN':\n",
    "        num_batches_vae = 1\n",
    "    print(\"config信息%s\" % FLAGS.graph_AE,num_batches_vae)\n",
    "\n",
    "    #读取及联数据\n",
    "    train_cascades, train_times = load_cascades(FLAGS.dataset, mode='train')\n",
    "    val_cascades, val_times = load_cascades(FLAGS.dataset, mode='val')\n",
    "    test_cascades, test_times = load_cascades(FLAGS.dataset, mode='test')\n",
    "\n",
    "    # 对数据进行截断,并且构建seed和剩余片段： Truncating input data based on max_seq_length.\n",
    "    train_examples, train_examples_times = get_data_set(train_cascades, train_times,\n",
    "                                                        max_len=FLAGS.max_seq_length,\n",
    "                                                        mode='train')\n",
    "    val_examples, val_examples_times = get_data_set(val_cascades, val_times,\n",
    "                                                    max_len=FLAGS.max_seq_length,\n",
    "                                                    mode='val')\n",
    "    test_examples, test_examples_times = get_data_set(test_cascades, test_times,\n",
    "                                                      max_len=FLAGS.max_seq_length,\n",
    "                                                      test_min_percent=FLAGS.test_min_percent,\n",
    "                                                      test_max_percent=FLAGS.test_max_percent,\n",
    "                                                      mode='test')\n",
    "\n",
    "    print(\"# nodes in graph\", num_nodes)\n",
    "    print(\"# train cascades\", len(train_cascades))\n",
    "\n",
    "    print(\"Init models\")\n",
    "    VGAE = InfVAESocial(X.shape[1], A, layers_config, mode='train', feats=X)\n",
    "    CoAtt = InfVAECascades(num_nodes + 1, train_examples, train_examples_times,\n",
    "                           val_examples, val_examples_times,\n",
    "                           test_examples, test_examples_times,\n",
    "                           logging=True, mode='feed')\n",
    "\n",
    "\n",
    "\n",
    "    # Initialize session\n",
    "    config = tf.compat.v1.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.compat.v1.Session(config=config)\n",
    "\n",
    "    # Init variables\n",
    "    print(\"Run global var initializer\")\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    print(\"Starting queue runners\")\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    z_vae_embeds = np.zeros([num_nodes + 1, FLAGS.latent_dim])\n",
    "    # print('初始值z：',z_vae_embeds)\n",
    "    logger.log(\"======VAE Pre-train=======\")\n",
    "    # Step 0: Pre-training using simple VAE on social network.\n",
    "    for epoch in range(FLAGS.pretrain_epochs):\n",
    "        losses = []\n",
    "        for b in range(0, num_batches_vae):\n",
    "            # Training step\n",
    "            input_feed = VGAE.construct_feed_dict(\n",
    "                v_sender_all=z_vae_embeds,\n",
    "                v_receiver_all=z_vae_embeds,\n",
    "                pre_train=True,\n",
    "                dropout=FLAGS.vae_dropout_rate)\n",
    "            _, vae_embeds, indices, train_loss = sess.run([\n",
    "                VGAE.opt_op, VGAE.z_mean, VGAE.node_indices, VGAE.vae_pre_train_loss\n",
    "            ], input_feed)\n",
    "            z_vae_embeds[indices] = vae_embeds\n",
    "            losses.append(train_loss)\n",
    "        epoch_loss = np.mean(losses)\n",
    "        logger.log(\"Mean VAE loss at epoch: %04d %.5f\" % (epoch + 1, epoch_loss))\n",
    "    logger.log(\"Pre-training completed\")\n",
    "\n",
    "    # Initial run to get embeddings.\n",
    "    logger.log(\"Initial run to get embeddings\")\n",
    "    for b in range(0, num_batches_vae):\n",
    "        t = time.time()\n",
    "        indices, z_val = sess.run([VGAE.node_indices, VGAE.z_mean])\n",
    "        z_vae_embeds[indices] = z_val\n",
    "        s = time.time()\n",
    "    # print('z shape:%s,  z[1]:%s' % z_vae_embeds.shape, z_vae_embeds[1])\n",
    "\n",
    "    val_loss_all = []\n",
    "    sender_embeds = np.copy(z_vae_embeds)\n",
    "    receiver_embeds = np.copy(z_vae_embeds)\n",
    "    for epoch in range(FLAGS.epochs):\n",
    "        # Train\n",
    "        # Step 1: VAE on Social Network.\n",
    "        losses = []\n",
    "        input_feed = VGAE.construct_feed_dict(\n",
    "            v_sender_all=sender_embeds,\n",
    "            v_receiver_all=receiver_embeds,\n",
    "            dropout=FLAGS.vae_dropout_rate)\n",
    "        for b in range(0, num_batches_vae):\n",
    "            # Training step\n",
    "            _, vae_embeds, indices, train_loss = sess.run([VGAE.opt_op, VGAE.z_mean, VGAE.node_indices,\n",
    "                                                           VGAE.social_loss], input_feed)\n",
    "            z_vae_embeds[indices] = vae_embeds\n",
    "            losses.append(train_loss)\n",
    "\n",
    "        epoch_loss = np.mean(losses)\n",
    "        logger.log(\"Mean VAE loss at epoch: %04d %.5f\" % (epoch + 1, epoch_loss))\n",
    "        \n",
    "        # # 保存模型到指定路径\n",
    "        # save_path = saver.save(sess, \"model_checkpoint/model_android_social.ckpt\")\n",
    "        # print(f\"Model saved in path: {save_path}\")\n",
    "\n",
    "        # Step 2: Diffusion Cascades\n",
    "        losses = []\n",
    "        input_feed = CoAtt.construct_feed_dict(z_vae_embeddings=z_vae_embeds)\n",
    "        # print('z shape:%s,  z[1]:%s' % z_vae_embeds.shape, z_vae_embeds[1])\n",
    "\n",
    "        for b in range(0, CoAtt.num_train_batches):\n",
    "            _, train_loss = sess.run([CoAtt.opt_op, CoAtt.diffusion_loss], input_feed)\n",
    "            losses.append(train_loss)\n",
    "        # re-assign based on updated sender, receiver embeddings.\n",
    "        sender_embeds = sess.run(CoAtt.sender_embeddings)\n",
    "        receiver_embeds = sess.run(CoAtt.receiver_embeddings)\n",
    "        epoch_loss = np.mean(losses)\n",
    "        logger.log(\"Mean Attention loss at epoch: %04d %.5f\" % (epoch + 1, epoch_loss))\n",
    "        # CoAtt.save(sess)\n",
    "        # print(\"sender_embeds的样子\",sender_embeds)\n",
    "        # print(\"z\",z_vae_embeds)\n",
    "        # # 保存所有变量到指定路径\n",
    "        # save_path = saver.save(sess, \"model_checkpoint/model_android_cascade.ckpt\")\n",
    "        # print(f\"cascade Model saved in path: {save_path}\")\n",
    "        \n",
    "        # Testing\n",
    "        if epoch % FLAGS.test_freq == 0:\n",
    "            # 先获取z_vae_embeds\n",
    "            # print('测试开始,先获取Z。num_batches_vae:', num_batches_vae)\n",
    "            # print(\"sender:\",sender_embeds)\n",
    "            # print(\"receiver:\",receiver_embeds)\n",
    "            input_feed = VGAE.construct_feed_dict(v_sender_all=sender_embeds,\n",
    "                                                  v_receiver_all=receiver_embeds, dropout=0.)\n",
    "            for _ in range(0, num_batches_vae):\n",
    "                vae_embeds, indices = sess.run([VGAE.z_mean, VGAE.node_indices], input_feed)\n",
    "                z_vae_embeds[indices] = vae_embeds\n",
    "\n",
    "            input_feed = CoAtt.construct_feed_dict(z_vae_embeddings=z_vae_embeds, is_test=True)\n",
    "\n",
    "            total_samples = 0\n",
    "            num_eval_k = len(CoAtt.k_list)\n",
    "            avg_map_scores, avg_recall_scores = [0.] * num_eval_k, [0.] * num_eval_k\n",
    "\n",
    "            all_outputs, all_targets, outs = [], [],[]\n",
    "            for b in range(0, CoAtt.num_test_batches):\n",
    "                recalls, maps, num_samples, decoder_outputs, decoder_targets = predict(\n",
    "                    sess, CoAtt, input_feed)\n",
    "                output = sess.run(CoAtt.outputs, feed_dict=input_feed)\n",
    "                all_outputs.append(decoder_outputs)\n",
    "                outs.append(output)\n",
    "                all_targets.append(decoder_targets)\n",
    "                avg_map_scores = list(\n",
    "                    map(operator.add, map(operator.mul, maps,\n",
    "                                          [num_samples] * num_eval_k), avg_map_scores))\n",
    "                avg_recall_scores = list(map(operator.add, map(operator.mul, recalls,\n",
    "                                                               [num_samples] * num_eval_k), avg_recall_scores))\n",
    "                total_samples += num_samples\n",
    "            all_outputs = np.vstack(all_outputs)\n",
    "            all_targets = np.vstack(all_targets)\n",
    "            outs = np.vstack(outs)\n",
    "\n",
    "            avg_map_scores = list(map(operator.truediv, avg_map_scores, [total_samples] * num_eval_k))\n",
    "            avg_recall_scores = list(map(operator.truediv, avg_recall_scores, [total_samples] * num_eval_k))\n",
    "\n",
    "            # print(\"预测值：%s \\n 真实值：%s\" % (all_outputs[0],all_targets[0]))\n",
    "            # print(\"是不是概率？？\",outs[0])\n",
    "\n",
    "            metrics = dict()\n",
    "            for k in range(0, num_eval_k):\n",
    "                K = CoAtt.k_list[k]\n",
    "                metrics[\"MAP@%d\" % K] = avg_map_scores[k]\n",
    "                metrics[\"Recall@%d\" % K] = avg_recall_scores[k]\n",
    "\n",
    "            logger.update_record(avg_map_scores[0], (all_outputs, all_targets, metrics))\n",
    "\n",
    "        # Validation\n",
    "        if epoch % FLAGS.val_freq == 0:\n",
    "            input_feed = VGAE.construct_feed_dict(\n",
    "                v_sender_all=sender_embeds, v_receiver_all=receiver_embeds, dropout=0.)\n",
    "            for b in range(0, num_batches_vae):\n",
    "                vae_embeds, indices = sess.run([VGAE.z_mean, VGAE.node_indices], input_feed)\n",
    "                z_vae_embeds[indices] = vae_embeds\n",
    "            losses = []\n",
    "            num_eval_k = len(CoAtt.k_list)\n",
    "            input_feed = CoAtt.construct_feed_dict(z_vae_embeddings=z_vae_embeds, is_val=True)\n",
    "            for b in range(0, CoAtt.num_val_batches):\n",
    "                val_loss = sess.run([CoAtt.diffusion_loss], input_feed)\n",
    "                losses.append(val_loss)\n",
    "            epoch_loss = np.mean(losses)\n",
    "            val_loss_all.append(epoch_loss)\n",
    "            logger.log(\"Validation Attention loss at epoch: %04d %.5f\" % (epoch + 1, epoch_loss))\n",
    "\n",
    "            # early stopping\n",
    "            if len(val_loss_all) >= FLAGS.early_stopping and val_loss_all[-1] > np.mean(\n",
    "                    val_loss_all[-(FLAGS.early_stopping + 1):-1]):\n",
    "                logger.log(\"Early stopping at epoch: %04d\" % (epoch + 1))\n",
    "                break\n",
    "\n",
    "    # print evaluation metrics\n",
    "    outputs, targets, metrics = logger.best_data\n",
    "    print(\"Evaluation metrics on test set:\")\n",
    "    pprint(metrics)\n",
    "\n",
    "    # stop queue runners\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "\n",
    "    # 保存最终模型\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, \"model_checkpoint/android_best\"+str(today.month)+str(today.day)+\".ckpt\")\n",
    "    print(f\"Final model saved in path: model_checkpoint/android_best.ckpt\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
